{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYSE & Blurr\n",
    "\n",
    "In this walkthrough guide we will train a machine learning model that predicts closing price of a stock based on historical data. We will transform time-series stock data into features to train this model. \n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Let's start by installing importing `Blurr`, `pandas` and other required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing blurr...\n",
      "pip 9.0.1 from /opt/conda/lib/python3.6/site-packages (python 3.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip uninstall blurr-dev --yes &> /dev/null\n",
    "\n",
    "print(\"installing blurr...\")\n",
    "!{sys.executable} -m pip install blurr-dev --quiet\n",
    "!{sys.executable} -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from blurr_util import print_head, validate, transform\n",
    "\n",
    "def print_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "This walkthrough is based on [New York Stock Exchange Data](https://www.kaggle.com/dgawlik/nyse/data) made available for [Kaggle challenges](https://www.kaggle.com/dgawlik/nyse).\n",
    "\n",
    "Let's have a peek at the available data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>symbol</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.839996</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>2163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119.980003</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>2386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.949997</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>2489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116.620003</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>2006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114.970001</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>1408600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        close       date        high         low        open symbol   volume\n",
       "0  125.839996 2016-01-05  126.250000  122.309998  123.430000   WLTW  2163600\n",
       "1  119.980003 2016-01-06  125.540001  119.940002  125.239998   WLTW  2386400\n",
       "2  114.949997 2016-01-07  119.739998  114.930000  116.379997   WLTW  2489500\n",
       "3  116.620003 2016-01-08  117.440002  113.500000  115.480003   WLTW  2006300\n",
       "4  114.970001 2016-01-11  117.330002  114.089996  117.010002   WLTW  1408600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = pd.read_json(\"./data/prices-split-adjusted.json\", lines=True)\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains data for each market day.\n",
    "\n",
    "Our **goal is to predict closing price** of a stock for any given day based on historical data. In order to do that, we need to transform our original data source into features that can be used for training.\n",
    "\n",
    "We'll calculate **moving averages** and other aggregate data for different **time windows**: one, three and seven days.\n",
    "\n",
    "## Blurr Templates\n",
    "\n",
    "We perform initial transformation of our data with [nyse-streaming-dtc.yml](./nyse-streaming-dtc.yml). Data is then aggregated by time using [nyse-window-dtc.yml](./nyse-window-dtc.yml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: 'Blurr:Transform:Streaming'\n",
      "Version: '2018-03-01'\n",
      "Description: New York Store Exchange Transformations\n",
      "Name: nyse\n",
      "\n",
      "Import:\n",
      "  - { Module: datetime, Identifiers: [ datetime ] }\n",
      "\n",
      "Identity: source.symbol\n",
      "\n",
      "Time: datetime.strptime(source.date, '%Y-%m-%d')\n",
      "\n",
      "Stores:\n",
      "  - Type: 'Blurr:Store:Memory'\n",
      "    Name: memory\n",
      "\n",
      "Aggregates:\n",
      "  - Type: 'Blurr:Aggregate:Block'\n",
      "    Name: stats\n",
      "    Store: memory\n",
      "    Split: True\n",
      "    When: source.symbol in ['AAPL', 'MSFT', 'GOOG', 'FB']\n",
      "    Fields:\n",
      "      - Name: close\n",
      "        Type: float\n",
      "        Value: source.close\n",
      "\n",
      "      - Name: volatility\n",
      "        Type: float\n",
      "        Value: (float(source.high) / float(source.low)) - 1\n",
      "\n",
      "      - Name: volume\n",
      "        Type: float\n",
      "        Value: source.volume\n"
     ]
    }
   ],
   "source": [
    "print_file('nyse-streaming-dtc.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming DTC**\n",
    "\n",
    "We're predicting values for tech companies only (Apple, Facebook, Microsoft, Google):\n",
    "\n",
    "```yaml\n",
    "When: source.symbol in ['AAPL', 'MSFT', 'GOOG', 'FB']\n",
    "```\n",
    "\n",
    "Each record in the original dataset represents a single day, which is the same we need to feed our window transformation. By setting `Split: True` we'll create a new aggregate for each single dataset record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Blurr:Transform:Window\n",
      "Version: '2018-03-01'\n",
      "Name: moving_averages\n",
      "\n",
      "SourceDTC: nyse\n",
      "\n",
      "Anchor:\n",
      "  Condition: nyse.stats.volatility < 0.04\n",
      "\n",
      "Aggregates:\n",
      "\n",
      "\n",
      "  - Type: Blurr:Aggregate:Window\n",
      "    Name: close\n",
      "    WindowType: count\n",
      "    WindowValue: 1\n",
      "    Source: nyse.stats\n",
      "    Fields:\n",
      "    - Name: value\n",
      "      Type: float\n",
      "      Value: anchor.close\n",
      "\n",
      "  - Type: Blurr:Aggregate:Window\n",
      "    Name: last\n",
      "    WindowType: count\n",
      "    WindowValue: -1\n",
      "    Source: nyse.stats\n",
      "    Fields:\n",
      "    - Name: close\n",
      "      Type: float\n",
      "      Value: source.close[0]\n",
      "    - Name: volume\n",
      "      Type: float\n",
      "      Value: source.volume[0]\n",
      "    - Name: volatility\n",
      "      Type: float\n",
      "      Value: source.volatility[0]\n",
      "\n",
      "  - Type: Blurr:Aggregate:Window\n",
      "    Name: last_3\n",
      "    WindowType: count\n",
      "    WindowValue: -3\n",
      "    Source: nyse.stats\n",
      "    Fields:\n",
      "    - Name: close_avg\n",
      "      Type: float\n",
      "      Value: sum(source.close) / len(source.close)\n",
      "    - Name: volume_avg\n",
      "      Type: float\n",
      "      Value: sum(source.volume) / len(source.volume)\n",
      "    - Name: volatility_avg\n",
      "      Type: float\n",
      "      Value: sum(source.volatility) / len(source.volatility)\n",
      "    - Name: max_volatility\n",
      "      Type: float\n",
      "      Value: max(source.volatility)\n",
      "    - Name: min_volatility\n",
      "      Type: float\n",
      "      Value: min(source.volatility)\n",
      "\n",
      "  - Type: Blurr:Aggregate:Window\n",
      "    Name: last_7\n",
      "    WindowType: count\n",
      "    WindowValue: -7\n",
      "    Source: nyse.stats\n",
      "    Fields:\n",
      "    - Name: close_avg\n",
      "      Type: float\n",
      "      Value: sum(source.close) / len(source.close)\n",
      "    - Name: volume_avg\n",
      "      Type: float\n",
      "      Value: sum(source.volume) / len(source.volume)\n",
      "    - Name: volatility_avg\n",
      "      Type: float\n",
      "      Value: sum(source.volatility) / len(source.volatility)\n",
      "    - Name: max_volatility\n",
      "      Type: float\n",
      "      Value: max(source.volatility)\n",
      "    - Name: min_volatility\n",
      "      Type: float\n",
      "      Value: min(source.volatility)\n"
     ]
    }
   ],
   "source": [
    "print_file('nyse-window-dtc.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window DTC**\n",
    "\n",
    "We'll use a very rough criteria to remove outliers: our model will only work when closing price changes less than a 4%:\n",
    "\n",
    "```yaml\n",
    "Anchor:\n",
    "  Condition: nyse.stats.volatility < 0.04\n",
    "```\n",
    "\n",
    "We're using [moving averages](https://www.investopedia.com/terms/m/movingaverage.asp) to generate features based on historical data about a stock:\n",
    "\n",
    "```yaml\n",
    "- Type: Blurr:Aggregate:Window\n",
    "    Name: last_7\n",
    "    WindowType: count\n",
    "    WindowValue: -7\n",
    "    Source: nyse.stats\n",
    "    Fields:\n",
    "    - Name: close_avg\n",
    "      Type: float\n",
    "      Value: sum(source.close) / len(source.close)\n",
    "```\n",
    "\n",
    "\n",
    "## Transforming Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running syntax validation on nyse-streaming-dtc.yml\n",
      "Document is valid\n",
      "Running syntax validation on nyse-window-dtc.yml\n",
      "Document is valid\n"
     ]
    }
   ],
   "source": [
    "validate('nyse-streaming-dtc.yml')\n",
    "validate('nyse-window-dtc.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our Streaming DTC only for informational purposes only, so we can preview the result of the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(log_files=[\"./data/prices-split-adjusted.json\"],\n",
    "          stream_dtc='./nyse-streaming-dtc.yml',\n",
    "          output_file=\"./nyse-streaming-dtc-out.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>_identity<th><th>_start_time<th><th>_end_time<th><th>close<th><th>volatility<th><th>volume<th></tr><tr><td>AAPL<td><td>2010-01-04T00:00:00<td><td>2010-01-04T00:00:00<td><td>30.5728568571<td><td>0.009982083954336085<td><td>123432400.0<td></tr><tr><td>AAPL<td><td>2010-01-05T00:00:00<td><td>2010-01-05T00:00:00<td><td>30.6257132857<td><td>0.010973036651542811<td><td>150476200.0<td></tr><tr><td>AAPL<td><td>2010-01-06T00:00:00<td><td>2010-01-06T00:00:00<td><td>30.1385707143<td><td>0.02125739461533116<td><td>138040000.0<td></tr><tr><td>AAPL<td><td>2010-01-07T00:00:00<td><td>2010-01-07T00:00:00<td><td>30.0828571428<td><td>0.014111461035875461<td><td>119282800.0<td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_head(\"./nyse-streaming-dtc-out.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(log_files=[\"./data/prices-split-adjusted.json\"],\n",
    "          stream_dtc='./nyse-streaming-dtc.yml',\n",
    "          window_dtc='./nyse-window-dtc.yml',\n",
    "          output_file=\"./nyse-processed-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now preview the data that will be used to **train our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close._identity</th>\n",
       "      <th>close.value</th>\n",
       "      <th>last._identity</th>\n",
       "      <th>last.close</th>\n",
       "      <th>last.volatility</th>\n",
       "      <th>last.volume</th>\n",
       "      <th>last_3._identity</th>\n",
       "      <th>last_3.close_avg</th>\n",
       "      <th>last_3.max_volatility</th>\n",
       "      <th>last_3.min_volatility</th>\n",
       "      <th>last_3.volatility_avg</th>\n",
       "      <th>last_3.volume_avg</th>\n",
       "      <th>last_7._identity</th>\n",
       "      <th>last_7.close_avg</th>\n",
       "      <th>last_7.max_volatility</th>\n",
       "      <th>last_7.min_volatility</th>\n",
       "      <th>last_7.volatility_avg</th>\n",
       "      <th>last_7.volume_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.092857</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.674286</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>148614900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.990953</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>1.253583e+08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.198979</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>129615200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.918571</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.092857</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>151473000.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.927619</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>1.385484e+08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.130408</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>133621000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.418571</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.918571</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>108223500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.895238</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>1.361038e+08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.029388</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.018263</td>\n",
       "      <td>127584900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.418571</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>148516900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.810000</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.022729</td>\n",
       "      <td>1.360711e+08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>29.926531</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>129081600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.247143</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>182501900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.019047</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>1.464141e+08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>30.017551</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>138112900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  close._identity  close.value last._identity  last.close  last.volatility  \\\n",
       "0            AAPL    30.092857           AAPL   29.674286         0.016229   \n",
       "1            AAPL    29.918571           AAPL   30.092857         0.033464   \n",
       "2            AAPL    29.418571           AAPL   29.918571         0.006889   \n",
       "3            AAPL    30.719999           AAPL   29.418571         0.027833   \n",
       "4            AAPL    30.247143           AAPL   30.719999         0.038361   \n",
       "\n",
       "   last.volume last_3._identity  last_3.close_avg  last_3.max_volatility  \\\n",
       "0  148614900.0             AAPL         29.990953               0.021828   \n",
       "1  151473000.0             AAPL         29.927619               0.033464   \n",
       "2  108223500.0             AAPL         29.895238               0.033464   \n",
       "3  148516900.0             AAPL         29.810000               0.033464   \n",
       "4  182501900.0             AAPL         30.019047               0.038361   \n",
       "\n",
       "   last_3.min_volatility  last_3.volatility_avg  last_3.volume_avg  \\\n",
       "0               0.014063               0.017373       1.253583e+08   \n",
       "1               0.016229               0.023840       1.385484e+08   \n",
       "2               0.006889               0.018861       1.361038e+08   \n",
       "3               0.006889               0.022729       1.360711e+08   \n",
       "4               0.006889               0.024361       1.464141e+08   \n",
       "\n",
       "  last_7._identity  last_7.close_avg  last_7.max_volatility  \\\n",
       "0             AAPL         30.198979               0.021828   \n",
       "1             AAPL         30.130408               0.033464   \n",
       "2             AAPL         30.029388               0.033464   \n",
       "3             AAPL         29.926531               0.033464   \n",
       "4             AAPL         30.017551               0.038361   \n",
       "\n",
       "   last_7.min_volatility  last_7.volatility_avg  last_7.volume_avg  \n",
       "0               0.009982               0.015492        129615200.0  \n",
       "1               0.010973               0.018847        133621000.0  \n",
       "2               0.006889               0.018263        127584900.0  \n",
       "3               0.006889               0.019203        129081600.0  \n",
       "4               0.006889               0.022667        138112900.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_out = pd.read_csv(\"./nyse-processed-data.csv\")\n",
    "window_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "We need to install Tensorflow and import all required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install tensorflow --quiet\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import h5py\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Source Data\n",
    "\n",
    "We next read data into a DataFrame. We're dropping unnecessary `_identity` columns and then applying a MinMax normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last.close</th>\n",
       "      <th>last.volatility</th>\n",
       "      <th>last.volume</th>\n",
       "      <th>last_3.close_avg</th>\n",
       "      <th>last_3.max_volatility</th>\n",
       "      <th>last_3.min_volatility</th>\n",
       "      <th>last_3.volatility_avg</th>\n",
       "      <th>last_3.volume_avg</th>\n",
       "      <th>last_7.close_avg</th>\n",
       "      <th>last_7.max_volatility</th>\n",
       "      <th>last_7.min_volatility</th>\n",
       "      <th>last_7.volatility_avg</th>\n",
       "      <th>last_7.volume_avg</th>\n",
       "      <th>adj close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.674286</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>148614900.0</td>\n",
       "      <td>29.990953</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>1.253583e+08</td>\n",
       "      <td>30.198979</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>129615200.0</td>\n",
       "      <td>30.092857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.092857</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>151473000.0</td>\n",
       "      <td>29.927619</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>1.385484e+08</td>\n",
       "      <td>30.130408</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>133621000.0</td>\n",
       "      <td>29.918571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.918571</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>108223500.0</td>\n",
       "      <td>29.895238</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>1.361038e+08</td>\n",
       "      <td>30.029388</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.018263</td>\n",
       "      <td>127584900.0</td>\n",
       "      <td>29.418571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.418571</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>148516900.0</td>\n",
       "      <td>29.810000</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.022729</td>\n",
       "      <td>1.360711e+08</td>\n",
       "      <td>29.926531</td>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>129081600.0</td>\n",
       "      <td>30.719999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.719999</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>182501900.0</td>\n",
       "      <td>30.019047</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>1.464141e+08</td>\n",
       "      <td>30.017551</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>138112900.0</td>\n",
       "      <td>30.247143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   last.close  last.volatility  last.volume  last_3.close_avg  \\\n",
       "0   29.674286         0.016229  148614900.0         29.990953   \n",
       "1   30.092857         0.033464  151473000.0         29.927619   \n",
       "2   29.918571         0.006889  108223500.0         29.895238   \n",
       "3   29.418571         0.027833  148516900.0         29.810000   \n",
       "4   30.719999         0.038361  182501900.0         30.019047   \n",
       "\n",
       "   last_3.max_volatility  last_3.min_volatility  last_3.volatility_avg  \\\n",
       "0               0.021828               0.014063               0.017373   \n",
       "1               0.033464               0.016229               0.023840   \n",
       "2               0.033464               0.006889               0.018861   \n",
       "3               0.033464               0.006889               0.022729   \n",
       "4               0.038361               0.006889               0.024361   \n",
       "\n",
       "   last_3.volume_avg  last_7.close_avg  last_7.max_volatility  \\\n",
       "0       1.253583e+08         30.198979               0.021828   \n",
       "1       1.385484e+08         30.130408               0.033464   \n",
       "2       1.361038e+08         30.029388               0.033464   \n",
       "3       1.360711e+08         29.926531               0.033464   \n",
       "4       1.464141e+08         30.017551               0.038361   \n",
       "\n",
       "   last_7.min_volatility  last_7.volatility_avg  last_7.volume_avg  adj close  \n",
       "0               0.009982               0.015492        129615200.0  30.092857  \n",
       "1               0.010973               0.018847        133621000.0  29.918571  \n",
       "2               0.006889               0.018263        127584900.0  29.418571  \n",
       "3               0.006889               0.019203        129081600.0  30.719999  \n",
       "4               0.006889               0.022667        138112900.0  30.247143  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_source_df():\n",
    "    data = pd.read_csv(\"./nyse-processed-data.csv\")\n",
    "    data[\"adj close\"] = data[\"close.value\"] # Moving close to the last column\n",
    "    data.drop(['close.value'], 1, inplace=True) \n",
    "    data.drop(['close._identity'], 1, inplace=True) \n",
    "    data.drop(['last._identity'], 1, inplace=True) \n",
    "    data.drop(['last_3._identity'], 1, inplace=True) \n",
    "    data.drop(['last_7._identity'], 1, inplace=True) \n",
    "    return data\n",
    "\n",
    "df = read_source_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last.close</th>\n",
       "      <th>last.volatility</th>\n",
       "      <th>last.volume</th>\n",
       "      <th>last_3.close_avg</th>\n",
       "      <th>last_3.max_volatility</th>\n",
       "      <th>last_3.min_volatility</th>\n",
       "      <th>last_3.volatility_avg</th>\n",
       "      <th>last_3.volume_avg</th>\n",
       "      <th>last_7.close_avg</th>\n",
       "      <th>last_7.max_volatility</th>\n",
       "      <th>last_7.min_volatility</th>\n",
       "      <th>last_7.volatility_avg</th>\n",
       "      <th>last_7.volume_avg</th>\n",
       "      <th>adj close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.136157</td>\n",
       "      <td>0.316018</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.054455</td>\n",
       "      <td>0.201752</td>\n",
       "      <td>0.085975</td>\n",
       "      <td>0.323094</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>0.043794</td>\n",
       "      <td>0.201177</td>\n",
       "      <td>0.121629</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.009015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009102</td>\n",
       "      <td>0.325537</td>\n",
       "      <td>0.322096</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.094569</td>\n",
       "      <td>0.244491</td>\n",
       "      <td>0.133054</td>\n",
       "      <td>0.357095</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.084360</td>\n",
       "      <td>0.233622</td>\n",
       "      <td>0.169251</td>\n",
       "      <td>0.428119</td>\n",
       "      <td>0.008794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008882</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>0.230123</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.094569</td>\n",
       "      <td>0.060209</td>\n",
       "      <td>0.096804</td>\n",
       "      <td>0.350794</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>0.084360</td>\n",
       "      <td>0.099912</td>\n",
       "      <td>0.160969</td>\n",
       "      <td>0.408652</td>\n",
       "      <td>0.008161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.263664</td>\n",
       "      <td>0.315810</td>\n",
       "      <td>0.008622</td>\n",
       "      <td>0.094569</td>\n",
       "      <td>0.060209</td>\n",
       "      <td>0.124962</td>\n",
       "      <td>0.350709</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.084360</td>\n",
       "      <td>0.099912</td>\n",
       "      <td>0.174304</td>\n",
       "      <td>0.413479</td>\n",
       "      <td>0.009808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.379350</td>\n",
       "      <td>0.388082</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.111452</td>\n",
       "      <td>0.060209</td>\n",
       "      <td>0.136846</td>\n",
       "      <td>0.377372</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.101433</td>\n",
       "      <td>0.099912</td>\n",
       "      <td>0.223483</td>\n",
       "      <td>0.442605</td>\n",
       "      <td>0.009210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   last.close  last.volatility  last.volume  last_3.close_avg  \\\n",
       "0    0.008573         0.136157     0.316018          0.008853   \n",
       "1    0.009102         0.325537     0.322096          0.008772   \n",
       "2    0.008882         0.033530     0.230123          0.008731   \n",
       "3    0.008249         0.263664     0.315810          0.008622   \n",
       "4    0.009896         0.379350     0.388082          0.008889   \n",
       "\n",
       "   last_3.max_volatility  last_3.min_volatility  last_3.volatility_avg  \\\n",
       "0               0.054455               0.201752               0.085975   \n",
       "1               0.094569               0.244491               0.133054   \n",
       "2               0.094569               0.060209               0.096804   \n",
       "3               0.094569               0.060209               0.124962   \n",
       "4               0.111452               0.060209               0.136846   \n",
       "\n",
       "   last_3.volume_avg  last_7.close_avg  last_7.max_volatility  \\\n",
       "0           0.323094          0.008550               0.043794   \n",
       "1           0.357095          0.008462               0.084360   \n",
       "2           0.350794          0.008332               0.084360   \n",
       "3           0.350709          0.008200               0.084360   \n",
       "4           0.377372          0.008317               0.101433   \n",
       "\n",
       "   last_7.min_volatility  last_7.volatility_avg  last_7.volume_avg  adj close  \n",
       "0               0.201177               0.121629           0.415200   0.009015  \n",
       "1               0.233622               0.169251           0.428119   0.008794  \n",
       "2               0.099912               0.160969           0.408652   0.008161  \n",
       "3               0.099912               0.174304           0.413479   0.009808  \n",
       "4               0.099912               0.223483           0.442605   0.009210  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_data(df):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    for column in df:\n",
    "        df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
    "    return df\n",
    "\n",
    "df = normalize_data(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns) # 5\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    train = result[:int(row), :] # 90% date, all features \n",
    "    \n",
    "    x_train = train[:, :-1] \n",
    "    y_train = train[:, -1][:,-1]\n",
    "    \n",
    "    x_test = result[int(row):, :-1] \n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00857277 0.13615713 0.31601844 0.00885281 0.05445501 0.20175198\n",
      "  0.08597492 0.32309379 0.00855009 0.04379389 0.20117653 0.12162913\n",
      "  0.41519997 0.00901468]\n",
      " [0.00910246 0.32553734 0.32209642 0.00877199 0.09456897 0.24449068\n",
      "  0.13305359 0.35709534 0.00846199 0.08436013 0.23362221 0.16925068\n",
      "  0.42811888 0.0087941 ]\n",
      " [0.00888191 0.03353024 0.23012291 0.00873067 0.09456897 0.0602086\n",
      "  0.09680371 0.35079354 0.0083322  0.08436013 0.09991156 0.16096873\n",
      "  0.40865214 0.0081613 ]\n",
      " [0.00824916 0.2636643  0.31581003 0.0086219  0.09456897 0.0602086\n",
      "  0.1249622  0.35070934 0.00820004 0.08436013 0.09991156 0.17430432\n",
      "  0.41347908 0.00980839]\n",
      " [0.0098961  0.37934987 0.38808184 0.00888866 0.11145169 0.0602086\n",
      "  0.13684608 0.37737152 0.00831699 0.1014332  0.09991156 0.22348327\n",
      "  0.44260549 0.00920994]\n",
      " [0.00929771 0.27514862 0.32542494 0.00902842 0.11145169 0.47344906\n",
      "  0.19020473 0.41587942 0.00831043 0.1014332  0.09991156 0.25352885\n",
      "  0.46155751 0.00854821]\n",
      " [0.00677035 0.53410046 0.46876427 0.00810783 0.16000192 0.49407089\n",
      "  0.2282758  0.45150619 0.00799552 0.15053085 0.09991156 0.34240971\n",
      "  0.51145746 0.00764422]\n",
      " [0.00860169 0.56635557 0.91577171 0.00806773 0.17012138 0.36878642\n",
      "  0.28187556 1.         0.00775639 0.16076441 0.61197278 0.47354837\n",
      "  0.85959315 0.00696079]\n",
      " [0.00574169 0.6480186  0.66238125 0.00699462 0.1957417  0.59951769\n",
      "  0.32935142 0.88972249 0.00682764 0.1866736  0.61197278 0.53397299\n",
      "  0.9836764  0.00613635]\n",
      " [0.00622438 0.22779519 0.39864499 0.00619557 0.1957417  0.40904065\n",
      "  0.25458424 0.68077137 0.00647786 0.1866736  0.61197278 0.52409667\n",
      "  1.         0.00634065]\n",
      " [0.00642867 0.12488547 0.3712472  0.00598714 0.1957417  0.22425072\n",
      "  0.20843195 0.57869876 0.0064283  0.1866736  0.37212687 0.44857055\n",
      "  0.978873   0.00694994]\n",
      " [0.00703791 0.28450177 0.32711302 0.00642283 0.08169479 0.22425072\n",
      "  0.12815344 0.4432297  0.00632761 0.1866736  0.37212687 0.46317398\n",
      "  0.926999   0.0056518 ]\n",
      " [0.00573989 0.34786712 0.40277884 0.00625998 0.10157453 0.22425072\n",
      "  0.15466994 0.44490004 0.0059634  0.1866736  0.37212687 0.42373905\n",
      "  0.79921114 0.00626833]\n",
      " [0.00635636 0.25434023 0.45203829 0.00623567 0.10157453 0.45670628\n",
      "  0.18325854 0.47754463 0.00563774 0.1866736  0.37212687 0.36615243\n",
      "  0.69874367 0.00602606]\n",
      " [0.00611411 0.17759362 0.25424726 0.00592516 0.10157453 0.31889615\n",
      "  0.1596491  0.44810236 0.00550218 0.1866736  0.37212687 0.33730914\n",
      "  0.61866658 0.00640031]\n",
      " [0.00648833 0.11298985 0.33644806 0.00617673 0.07223214 0.20289034\n",
      "  0.10777916 0.42130063 0.00561047 0.09144469 0.33668097 0.2385624\n",
      "  0.54805342 0.00620686]\n",
      " [0.00629489 0.09019014 0.19687788 0.00615607 0.04815426 0.16195002\n",
      "  0.0715285  0.31820008 0.0056207  0.09144469 0.26874364 0.21316554\n",
      "  0.50434074 0.0068487 ]\n",
      " [0.00693667 0.28001239 0.29256546 0.00643255 0.08028632 0.16195002\n",
      "  0.09414651 0.333683   0.00569438 0.09144469 0.26874364 0.2417963\n",
      "  0.4872944  0.00715786]\n",
      " [0.00724581 0.3029308  0.34845367 0.00668715 0.08747657 0.16195002\n",
      "  0.13609278 0.338534   0.00572453 0.09144469 0.26874364 0.24519762\n",
      "  0.49191783 0.00770388]\n",
      " [0.00779178 0.076152   0.28905235 0.00719028 0.08747657 0.13674242\n",
      "  0.13299262 0.37577818 0.00602214 0.07718776 0.22691356 0.19504895\n",
      "  0.46727908 0.0075502 ]\n",
      " [0.00763811 0.14656406 0.23198516 0.00742605 0.08747657 0.13674242\n",
      "  0.10352209 0.3513     0.00620804 0.07718776 0.22691356 0.17515741\n",
      "  0.41960474 0.0076189 ]\n",
      " [0.00770681 0.12025627 0.22476989 0.007581   0.03841929 0.13674242\n",
      "  0.06318052 0.30132412 0.00643905 0.07718776 0.22691356 0.16457503\n",
      "  0.41321849 0.0073911 ]] 0.007165095824298657\n"
     ]
    }
   ],
   "source": [
    "window = 22\n",
    "X_train, y_train, X_test, y_test = load_data(df, window)\n",
    "print (X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.021618127822875977\n"
     ]
    }
   ],
   "source": [
    "model = build_model([len(df.columns), window,1]) # 14 is number of features??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4824 samples, validate on 536 samples\n",
      "Epoch 1/90\n",
      "4824/4824 [==============================] - 11s 2ms/step - loss: 0.0696 - acc: 2.0730e-04 - val_loss: 4.7455e-04 - val_acc: 0.0019\n",
      "Epoch 2/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0077 - acc: 2.0730e-04 - val_loss: 7.0672e-04 - val_acc: 0.0019\n",
      "Epoch 3/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0049 - acc: 2.0730e-04 - val_loss: 4.5533e-04 - val_acc: 0.0019\n",
      "Epoch 4/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0035 - acc: 2.0730e-04 - val_loss: 1.8529e-04 - val_acc: 0.0019\n",
      "Epoch 5/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0024 - acc: 2.0730e-04 - val_loss: 8.0141e-04 - val_acc: 0.0019\n",
      "Epoch 6/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0021 - acc: 2.0730e-04 - val_loss: 5.4182e-04 - val_acc: 0.0019\n",
      "Epoch 7/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0020 - acc: 2.0730e-04 - val_loss: 2.2904e-04 - val_acc: 0.0019\n",
      "Epoch 8/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0018 - acc: 2.0730e-04 - val_loss: 2.2079e-04 - val_acc: 0.0019\n",
      "Epoch 9/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0016 - acc: 2.0730e-04 - val_loss: 2.1274e-04 - val_acc: 0.0019\n",
      "Epoch 10/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0015 - acc: 2.0730e-04 - val_loss: 1.6029e-04 - val_acc: 0.0019\n",
      "Epoch 11/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 1.5692e-04 - val_acc: 0.0019\n",
      "Epoch 12/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 1.4523e-04 - val_acc: 0.0019\n",
      "Epoch 13/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 1.1476e-04 - val_acc: 0.0019\n",
      "Epoch 14/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 9.9456e-05 - val_acc: 0.0019\n",
      "Epoch 15/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0014 - acc: 2.0730e-04 - val_loss: 8.0951e-05 - val_acc: 0.0019\n",
      "Epoch 16/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0014 - acc: 2.0730e-04 - val_loss: 1.0983e-04 - val_acc: 0.0019\n",
      "Epoch 17/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 9.5306e-05 - val_acc: 0.0019\n",
      "Epoch 18/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0012 - acc: 2.0730e-04 - val_loss: 7.3990e-05 - val_acc: 0.0019\n",
      "Epoch 19/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0011 - acc: 2.0730e-04 - val_loss: 6.9649e-05 - val_acc: 0.0019\n",
      "Epoch 20/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 1.1382e-04 - val_acc: 0.0019\n",
      "Epoch 21/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 6.8088e-05 - val_acc: 0.0019\n",
      "Epoch 22/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0011 - acc: 2.0730e-04 - val_loss: 8.9819e-05 - val_acc: 0.0019\n",
      "Epoch 23/90\n",
      "4824/4824 [==============================] - 11s 2ms/step - loss: 0.0011 - acc: 2.0730e-04 - val_loss: 7.0369e-05 - val_acc: 0.0019\n",
      "Epoch 24/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 9.8443e-04 - acc: 2.0730e-04 - val_loss: 6.9158e-05 - val_acc: 0.0019\n",
      "Epoch 25/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 9.0423e-04 - acc: 2.0730e-04 - val_loss: 6.2074e-05 - val_acc: 0.0019\n",
      "Epoch 26/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 9.5959e-04 - acc: 2.0730e-04 - val_loss: 6.4377e-05 - val_acc: 0.0019\n",
      "Epoch 27/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 9.8613e-04 - acc: 2.0730e-04 - val_loss: 5.8499e-05 - val_acc: 0.0019\n",
      "Epoch 28/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 2.0730e-04 - val_loss: 1.1444e-04 - val_acc: 0.0019\n",
      "Epoch 29/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0011 - acc: 2.0730e-04 - val_loss: 5.2168e-05 - val_acc: 0.0019\n",
      "Epoch 30/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 9.3177e-04 - acc: 2.0730e-04 - val_loss: 5.4478e-05 - val_acc: 0.0019\n",
      "Epoch 31/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 9.0335e-04 - acc: 2.0730e-04 - val_loss: 4.6675e-05 - val_acc: 0.0019\n",
      "Epoch 32/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.7022e-04 - acc: 2.0730e-04 - val_loss: 4.9327e-05 - val_acc: 0.0019\n",
      "Epoch 33/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0010 - acc: 2.0730e-04 - val_loss: 5.5473e-05 - val_acc: 0.0019\n",
      "Epoch 34/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 0.0011 - acc: 2.0730e-04 - val_loss: 7.1424e-05 - val_acc: 0.0019\n",
      "Epoch 35/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.8308e-04 - acc: 2.0730e-04 - val_loss: 8.1753e-05 - val_acc: 0.0019\n",
      "Epoch 36/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.3280e-04 - acc: 2.0730e-04 - val_loss: 5.9885e-05 - val_acc: 0.0019\n",
      "Epoch 37/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.0600e-04 - acc: 2.0730e-04 - val_loss: 4.1869e-05 - val_acc: 0.0019\n",
      "Epoch 38/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.9706e-04 - acc: 2.0730e-04 - val_loss: 4.1063e-05 - val_acc: 0.0019\n",
      "Epoch 39/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.4497e-04 - acc: 2.0730e-04 - val_loss: 4.7236e-05 - val_acc: 0.0019\n",
      "Epoch 40/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.8330e-04 - acc: 2.0730e-04 - val_loss: 5.2232e-05 - val_acc: 0.0019\n",
      "Epoch 41/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.0303e-04 - acc: 2.0730e-04 - val_loss: 4.4292e-05 - val_acc: 0.0019\n",
      "Epoch 42/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.1086e-04 - acc: 2.0730e-04 - val_loss: 4.4713e-05 - val_acc: 0.0019\n",
      "Epoch 43/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.5069e-04 - acc: 2.0730e-04 - val_loss: 4.2755e-05 - val_acc: 0.0019\n",
      "Epoch 44/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.9829e-04 - acc: 2.0730e-04 - val_loss: 6.5162e-05 - val_acc: 0.0019\n",
      "Epoch 45/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.2123e-04 - acc: 2.0730e-04 - val_loss: 3.2332e-05 - val_acc: 0.0019\n",
      "Epoch 46/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.9003e-04 - acc: 2.0730e-04 - val_loss: 3.0249e-05 - val_acc: 0.0019\n",
      "Epoch 47/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.5335e-04 - acc: 2.0730e-04 - val_loss: 4.2112e-05 - val_acc: 0.0019\n",
      "Epoch 48/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.1347e-04 - acc: 2.0730e-04 - val_loss: 6.4235e-05 - val_acc: 0.0019\n",
      "Epoch 49/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.8622e-04 - acc: 2.0730e-04 - val_loss: 4.6982e-05 - val_acc: 0.0019\n",
      "Epoch 50/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.5542e-04 - acc: 2.0730e-04 - val_loss: 3.6787e-05 - val_acc: 0.0019\n",
      "Epoch 51/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.1791e-04 - acc: 2.0730e-04 - val_loss: 4.6451e-05 - val_acc: 0.0019\n",
      "Epoch 52/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.2080e-04 - acc: 2.0730e-04 - val_loss: 3.7713e-05 - val_acc: 0.0019\n",
      "Epoch 53/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.3010e-04 - acc: 2.0730e-04 - val_loss: 4.9865e-05 - val_acc: 0.0019\n",
      "Epoch 54/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.3853e-04 - acc: 2.0730e-04 - val_loss: 3.1597e-05 - val_acc: 0.0019\n",
      "Epoch 55/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.3321e-04 - acc: 2.0730e-04 - val_loss: 4.6294e-05 - val_acc: 0.0019\n",
      "Epoch 56/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.0276e-04 - acc: 2.0730e-04 - val_loss: 3.4842e-05 - val_acc: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.2748e-04 - acc: 2.0730e-04 - val_loss: 3.1792e-05 - val_acc: 0.0019\n",
      "Epoch 58/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.9650e-04 - acc: 2.0730e-04 - val_loss: 2.3480e-05 - val_acc: 0.0019\n",
      "Epoch 59/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.5818e-04 - acc: 2.0730e-04 - val_loss: 2.6155e-05 - val_acc: 0.0019\n",
      "Epoch 60/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.4355e-04 - acc: 2.0730e-04 - val_loss: 3.9931e-05 - val_acc: 0.0019\n",
      "Epoch 61/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.9218e-04 - acc: 2.0730e-04 - val_loss: 2.6625e-05 - val_acc: 0.0019\n",
      "Epoch 62/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.0305e-04 - acc: 2.0730e-04 - val_loss: 2.4278e-05 - val_acc: 0.0019\n",
      "Epoch 63/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.6190e-04 - acc: 2.0730e-04 - val_loss: 3.0325e-05 - val_acc: 0.0019\n",
      "Epoch 64/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.1397e-04 - acc: 2.0730e-04 - val_loss: 4.1825e-05 - val_acc: 0.0019\n",
      "Epoch 65/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.1562e-04 - acc: 2.0730e-04 - val_loss: 3.5512e-05 - val_acc: 0.0019\n",
      "Epoch 66/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.7568e-04 - acc: 2.0730e-04 - val_loss: 2.3151e-05 - val_acc: 0.0019\n",
      "Epoch 67/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.8501e-04 - acc: 2.0730e-04 - val_loss: 2.5921e-05 - val_acc: 0.0019\n",
      "Epoch 68/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.3153e-04 - acc: 2.0730e-04 - val_loss: 2.7181e-05 - val_acc: 0.0019\n",
      "Epoch 69/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.9018e-04 - acc: 2.0730e-04 - val_loss: 2.4346e-05 - val_acc: 0.0019\n",
      "Epoch 70/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.3784e-04 - acc: 2.0730e-04 - val_loss: 2.4125e-05 - val_acc: 0.0019\n",
      "Epoch 71/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.2458e-04 - acc: 2.0730e-04 - val_loss: 2.6909e-05 - val_acc: 0.0019\n",
      "Epoch 72/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 8.6947e-04 - acc: 2.0730e-04 - val_loss: 4.2620e-05 - val_acc: 0.0019\n",
      "Epoch 73/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.6787e-04 - acc: 2.0730e-04 - val_loss: 2.6549e-05 - val_acc: 0.0019\n",
      "Epoch 74/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.0502e-04 - acc: 2.0730e-04 - val_loss: 5.7656e-05 - val_acc: 0.0019\n",
      "Epoch 75/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.9682e-04 - acc: 2.0730e-04 - val_loss: 4.7214e-05 - val_acc: 0.0019\n",
      "Epoch 76/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 6.3898e-04 - acc: 2.0730e-04 - val_loss: 2.2473e-05 - val_acc: 0.0019\n",
      "Epoch 77/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 5.9504e-04 - acc: 2.0730e-04 - val_loss: 2.8835e-05 - val_acc: 0.0019\n",
      "Epoch 78/90\n",
      "4824/4824 [==============================] - 13s 3ms/step - loss: 5.8155e-04 - acc: 2.0730e-04 - val_loss: 3.1404e-05 - val_acc: 0.0019\n",
      "Epoch 79/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 7.2907e-04 - acc: 2.0730e-04 - val_loss: 2.5825e-05 - val_acc: 0.0019\n",
      "Epoch 80/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 6.2862e-04 - acc: 2.0730e-04 - val_loss: 2.5955e-05 - val_acc: 0.0019\n",
      "Epoch 81/90\n",
      "4824/4824 [==============================] - 14s 3ms/step - loss: 5.9228e-04 - acc: 2.0730e-04 - val_loss: 3.3075e-05 - val_acc: 0.0019\n",
      "Epoch 82/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 5.7891e-04 - acc: 2.0730e-04 - val_loss: 4.6648e-05 - val_acc: 0.0019\n",
      "Epoch 83/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 6.9570e-04 - acc: 2.0730e-04 - val_loss: 3.7627e-05 - val_acc: 0.0019\n",
      "Epoch 84/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 6.3099e-04 - acc: 2.0730e-04 - val_loss: 3.0128e-05 - val_acc: 0.0019\n",
      "Epoch 85/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 6.6882e-04 - acc: 2.0730e-04 - val_loss: 2.6892e-05 - val_acc: 0.0019\n",
      "Epoch 86/90\n",
      "4824/4824 [==============================] - 12s 2ms/step - loss: 6.7914e-04 - acc: 2.0730e-04 - val_loss: 2.9007e-05 - val_acc: 0.0019\n",
      "Epoch 87/90\n",
      "4824/4824 [==============================] - 11s 2ms/step - loss: 6.1556e-04 - acc: 2.0730e-04 - val_loss: 2.6451e-05 - val_acc: 0.0019\n",
      "Epoch 88/90\n",
      "4824/4824 [==============================] - 11s 2ms/step - loss: 5.8226e-04 - acc: 2.0730e-04 - val_loss: 2.3507e-05 - val_acc: 0.0019\n",
      "Epoch 89/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 5.6339e-04 - acc: 2.0730e-04 - val_loss: 2.5183e-05 - val_acc: 0.0019\n",
      "Epoch 90/90\n",
      "4824/4824 [==============================] - 10s 2ms/step - loss: 5.5426e-04 - acc: 2.0730e-04 - val_loss: 4.1876e-05 - val_acc: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10bbcd0780>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=90,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595, 1)\n"
     ]
    }
   ],
   "source": [
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "print (p.shape)\n",
    "for u in range(len(y_test)):\n",
    "    # pr = prediction day u\n",
    "    pr = p[u][0]\n",
    "    # (y_test day u / pr) - 1\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_source_df()\n",
    "\n",
    "def denormalize(df, normalized_value): \n",
    "    df = df['adj close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new\n",
    "\n",
    "newp = denormalize(df, p)\n",
    "newy_test = denormalize(df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00028 MSE (0.02 RMSE)\n",
      "Test Score: 0.00002 MSE (0.00 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00027845214715636023, 1.8041758827900342e-05)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot(newp,color='red', label='Prediction')\n",
    "plt2.plot(newy_test,color='blue', label='Actual')\n",
    "plt2.legend(loc='best')\n",
    "plt2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
